{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lab 5: A 2-dimensional classification problem\n",
    "In this session we will work with a set of data points in a 2-D plane. The task is to classify the points based on their coordinates and assign them to two different categories. The two categories are indicated with an integer with values 0 and 1. Each point on the plane belongs to a unique category.\n",
    "\n",
    "The goal of the classification algorithm is to define a *prediction function* that predicts the category of a point based on its coordinates. The accuracy of our prediction is the ratio between the number of times we get the correct result and the total number of points.\n",
    "\n",
    "In first part, we define the dataset and separate the categories in a simple way by drawing a straight line on the plane. \n",
    "\n",
    "In the second part we use a neural network with one layer to find a more accurate solution\n",
    "\n",
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Helper functions\n",
    "These two helper functions are used throughout the project. The input argument is a prediction function (*pred_func*). \n",
    "*pred_func* is a function that takes an array of points and returns an array of 0 and 1 based on the prediction. Examples are provided below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# Input: pred_func. A function defined that operates on an array of points. \n",
    "#                   pred_func is expected to return an array of 0 and 1 \n",
    "#                   for each point passed in the input array a 0 or 1 is returned based on the condition\n",
    "#                   in the function\n",
    "\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    \n",
    "# Helper function to calculate how often we get the prediction right\n",
    "# This is the accuracy of our classification e.g. N_right/N_tot\n",
    "def accuracy(pred_func):\n",
    "    yhat=pred_func(X)\n",
    "    return 1.-np.count_nonzero(yhat-y)/len(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1: Prepare dataset and simple classification\n",
    "\n",
    "### Dataset preparation\n",
    "Data points are generated using the *generate_data* function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# This cell contains the code to prepare the datasets\n",
    "# It uses sklearn.make_moons that generate sets of data sets to train and test classification algorithms\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Generate a dataset and plot it\n",
    "#     Npoints is the number of points in the dataset\n",
    "#     rndm is a random seed so different datasets can be generated\n",
    "#     noise is a parameter controlling how much points are scattered\n",
    "def generate_data(Npoints,rndm=0,noise=0.20):\n",
    "    np.random.seed(rndm)\n",
    "    X, y = sklearn.datasets.make_moons(Npoints, noise=noise)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Generate a dataset and plot it\n",
    "X,y=generate_data(500)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral) #1 is blue, 0 is red\n",
    "\n",
    "# Print the data (first 10 elements)\n",
    "print(\"X=\",X[:10,:])\n",
    "print(\"y=\",y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Making a prediction and plotting a boundary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Here we show how a boundary can be defined and plotted\n",
    "\n",
    "# First a simple function of X to illustrate the mechanism. \n",
    "# In this example we define a prediction that all points above 0 on the vertical axis belong to category 0\n",
    "def simplePred(x):\n",
    "    return np.where(x[:,1]>0., 0, 1)\n",
    "\n",
    "plot_decision_boundary(simplePred)\n",
    "\n",
    "# Calculate the accuracy of this predition\n",
    "print(\"Accuracy=\",accuracy(simplePred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# The example above is impractical, as if we want to use a different classification \n",
    "# (eg points above 0.5 are assigned to category 0) we need to write a new function.\n",
    "\n",
    "# A simple way to separate the two categories\n",
    "def predConst(x,C):\n",
    "    return np.where(x[:,1]>C, 0, 1)\n",
    "\n",
    "# Use lambda to turn the function into a function of just x\n",
    "plot_decision_boundary(lambda x: predConst(x,0.5))\n",
    "print(\"Accuracy\",accuracy(lambda x: predConst(x,0.5)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 1. Optimise the constant prediction\n",
    "Loop over a value of C in the range (-1:1) in steps of 0.01 and find the value with the highest accuracy. Plot the decision boundary for this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2: Train a linear model\n",
    "Write a predLinear function that takes two arguments and separates the plane usign a straight line (a*x+b). The points above corresponds to category 0 and those below to category 1\n",
    "\n",
    "Scan the parameters a and b and find the values that yield the highest accuracy. Print the values of a and b and plot the boundary graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Function to define a linear model\n",
    "def predLinear(x,a,b):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train a neural network\n",
    "The boundary between the red and blue areas can be identified using a neural network, as discussed in the lecture earlier. The following helper functions lay out the calculation needed to evaluate the loss function, its derivatives and to find the parameters of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# These are parameter that are used in the training\n",
    "# \n",
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Calculate the value of the output layers a2\n",
    "def calcProbsNN(model,x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    # This is the softmax function\n",
    "    exp_scores = np.exp(z2)\n",
    "    # This is a2=yhat, eg the values of the output layer\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)    \n",
    "    return probs\n",
    "    \n",
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    probs = calcProbsNN(model,x)   \n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "# See notes from this morning lecture\n",
    "#\n",
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    probs = calcProbsNN(model,X)   \n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "\n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "          print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model building\n",
    "This example code uses the helpers above to run a neural network with 3 hidden layers and lots the decision boundary and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model(3, print_loss=True,num_passes=500)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: predict(model, x))\n",
    "plt.title(\"Decision Boundary for hidden layer size 3\")\n",
    "print(\"Accuracy\",accuracy(lambda x: predict(model,x)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 3\n",
    "Use the code above to plot the boundaries for different sizes of the hidden layer (1, 2, 3, 4, 5, 10, 50). Evaluate the accuracy for each of the cases and report that as a table or as a graph\n",
    "\n",
    "### Exercise 4\n",
    "For the case with a hidden layer of dimension 4, print the matrices $W_1$, $W_2$ and the bias vectors $b_1$ and $b_2$ that are obtained at the end of the training. Comment on their dimensionality. \n",
    "\n",
    "### Exercise 5\n",
    "Generate a \"test data set\" with 1000 points, which is statistically independent from the training set by using a different random seed. Evaluate the accuracy as a function of the hidden layer dimension on this test data set. **Note* the test data set should not be used for training\n",
    "\n",
    "### Exercise 6\n",
    "Repeat the exercises above using a larger training set of 2000 events. In particular evaluate the efficiency using the test set defined in exercise 5\n",
    "\n",
    "### Exercise 7\n",
    "How would you modify the code to classify elements with 3 or more categories? How would you modify the code to allow for more input parameters? Please provide a written description with code examples, but a full working code is not needed. The purpose of this exercise is for you to reflect on how to implement a more generic ANN, which we will use later in the course. \n",
    "\n",
    "### For PHY428. \n",
    "   * Generate test sets with different value of noise (0.4 and 0.6) and evaluate the efficiency using the model trained in exercise 5\n",
    "   * Optimise the minimisation of the loss function. Plot the value of the loss function vs the loop number and find a way to optimise the number of loops by stopping when a given a \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}